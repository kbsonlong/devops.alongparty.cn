[{"categories":["kubernetes"],"content":"什么是弹性伸缩？弹性伸缩和成本优化是何关系？ 应该如何做好企业级弹性伸缩与成本优化建设？ ","date":"2022-08-01","objectID":"/%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E5%92%8C%E4%BC%98%E5%8C%96%E5%BB%BA%E8%AE%BE/:0:0","tags":["kubernetes"],"title":"[转载]企业级弹性伸缩和优化建设","uri":"/%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E5%92%8C%E4%BC%98%E5%8C%96%E5%BB%BA%E8%AE%BE/"},{"categories":["kubernetes"],"content":"一 背景 传统意义上来讲，弹性伸缩主要解决的问题是容量规划与实际负载的矛盾, 这矛盾通常因为资源使用普遍具有以下几个问题导致： （1）在线服务申请资源时考虑到突发流量和服务稳定性，预留大量的 buffer 资源，造成资源申请量普遍远超实际使用量。 （2）大部分在线服务的潮汐现象、波峰波谷特征非常明显，保留过多常态资源造成巨大浪费。 （3）开发和运维评估和配置的资源规格不合理，并且动态更新不及时。 随着云原生技术的发展，容器云的弹性能力进入大众视野，弹性伸缩也成为各大服务器厂商的标准配置，在高速发展的互联网企业中，弹性既能支撑业务优雅面对突发的大流量、亦能在业务低峰时伸缩资源维持低成本运行，尤其在当前的大环境下， 降本增效的弹性能力建设是必然。那应该如何打造我们的弹性伸缩能力、又如何利用弹性的能力解决高昂的成本问题？本文根据笔者过去的实践和思考， 简单阐述对计算资源弹性、容器云kubernetes[1]弹性、混合云弹性、Serverless[2]等的一些心得体会。 阅者受益： 计算资源弹性能力方向建设 容器云集群弹性方向建设 容器云应用弹性方向建设 混合云弹性建设 Serverless方向建设 ","date":"2022-08-01","objectID":"/%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E5%92%8C%E4%BC%98%E5%8C%96%E5%BB%BA%E8%AE%BE/:1:0","tags":["kubernetes"],"title":"[转载]企业级弹性伸缩和优化建设","uri":"/%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E5%92%8C%E4%BC%98%E5%8C%96%E5%BB%BA%E8%AE%BE/"},{"categories":["kubernetes"],"content":"二 计算资源弹性能力建设 业务还未部署在容器云的时候， 资源是私有云或公有云的虚拟机为主，设计弹性平台的落地产出就是提升虚拟机的资源利用率,降低服务器成本和动态感知业务增长,实现平滑扩容 弹性平台建设 弹性平台基于应用资源画像的洞察-分析-优化方向建设， 整体架构包括可视化端层、计算层（数据获取、应用分析、弹性决策设定）、实施自动化，核心就是通过对资源使用率监控数据进行洞察分析，感知不同业务线的资源使用情况，对资源持续低负载或高负载有全局视角， 判断应用是否需要进行伸缩活动， 因为虚拟机资源的伸缩活动是分钟级（5分钟）以上的， 做不到容器云的秒级弹性， 所以还要考虑虚机资源池化 平台可视化层主要提供资源利用率大盘、应用弹性伸缩建议、业务健康度监控、事件日志审计、管理后台入口 计算模块主要提供数据获取、数据分析、伸缩弹性判定决策树 自动化模块主要提供伸缩事件通知、流量摘除、伸缩资源变更、业务部署、流量拉入等 计算资源弹性平台架构 这里主要对决策树稍加阐述， 设定的算法逻辑主要考虑以下几点： 设计资源利用率升降配决策树 降配时类型优先级：通用型 \u003e 计算型 \u003e 网络增强型 升配时逆序 确定类型降配达标条件 主机：CPU利用率达到xx%，内存利用率达到xx% 设计优先横向实例数量扩缩 \u003e 纵向规格变更 弹性扩容冷却期，在发生弹性扩容24h内， 不再进行缩容操作 考虑特殊情况， 白名单机制，譬如电商618、双十一等就需要提前扩容 当然资源成本优化也会考虑云商折扣、资源的类型譬如包年包月、按量付费、Spot抢占实例的组合方式等，Spot抢占实例在容器云中发挥的作用比较大，会在下文介绍 ","date":"2022-08-01","objectID":"/%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E5%92%8C%E4%BC%98%E5%8C%96%E5%BB%BA%E8%AE%BE/:2:0","tags":["kubernetes"],"title":"[转载]企业级弹性伸缩和优化建设","uri":"/%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E5%92%8C%E4%BC%98%E5%8C%96%E5%BB%BA%E8%AE%BE/"},{"categories":["kubernetes"],"content":"三 容器云集群层面优化 随着云原生的发展， Kubernetes容器编排成为业务上云的首选。业界在容器上建设的路线基本遵循 Docker \u003e Kubernetes \u003e ServiceMesh \u003e Serverless 。因为容器云是当下的主流， 所以下面会用比较大的篇幅来介绍容器云的弹性伸缩及优化方式 容器云弹性分为集群弹性和容器应用弹性两个层面。容器应用层面的弹性属于K8S的HPA或者VPA资源管理的范畴， 集群层面的弹性属于k8S的节点伸缩、动态调度等，本节从集群弹性讲起，主要包括以下内容： 集群弹性Cluster Autoscaler[3] Node节点超卖 集群自定义调度[4] 集群重调度[5] 离线和在线业务混部 Spot[6]抢占实例替换 ","date":"2022-08-01","objectID":"/%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E5%92%8C%E4%BC%98%E5%8C%96%E5%BB%BA%E8%AE%BE/:3:0","tags":["kubernetes"],"title":"[转载]企业级弹性伸缩和优化建设","uri":"/%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E5%92%8C%E4%BC%98%E5%8C%96%E5%BB%BA%E8%AE%BE/"},{"categories":["kubernetes"],"content":"3.0、集群资源构成 相比虚机类的弹性， 容器云环境的弹性是更为智能、高效的、可落地的。以 CPU 资源为例，一个 Kubernetes 集群的Node节点资源组成结构大致如下： Kubernetes Node 资源构成 Node Capacity是Node的所有硬件资源 Kube-reserved是给kube组件预留的资源 System-reserved是给System进程预留的资源 Eviction-threshold（阈值）是kubelet eviction(收回)的阈值设定 Allocatable才是真正scheduler调度Pod时的参考值 Node Allocatable = Node Capacity - kube-reserved - system-reserved - eviction-threshold 资源碎片因为node配置低、大资源pod不能调度、cpu满足但mem不满足调度等 通常业务申请的资源多，但POD实际利用率低，也导致Node利用率低 3.1、Kubernetes弹性优化总览 Kubernetes弹性优化总览 ","date":"2022-08-01","objectID":"/%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E5%92%8C%E4%BC%98%E5%8C%96%E5%BB%BA%E8%AE%BE/:3:1","tags":["kubernetes"],"title":"[转载]企业级弹性伸缩和优化建设","uri":"/%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E5%92%8C%E4%BC%98%E5%8C%96%E5%BB%BA%E8%AE%BE/"},{"categories":["kubernetes"],"content":"3.2、集群弹性CA Kubernetes AutoScaling 提供多种机制来满足 Pod 自动伸缩需求： Pod 级别的自动伸缩：包括Horizontal Pod Autoscaler[8]（HPA）和Vertical Pod Autoscaler[9]。其中 HPA 会基于 kubernetes 集群内置资源指标或者自定义指标来计算 Pod 副本需求数并自动伸缩，VPA 会基于 Pod 在 CPU/Memory 资源历史使用详情来计算 Pod 合理的资源请求并驱逐 Pod 以更新资源配合； Node 级别的自动伸缩：Cluster Autoscaler[3]会综合考虑 Pod 部署挂起或集群容量等信息确定集群节点资源并相应调整集群 Node 数量。 我先聊一聊集群Node级别的弹性伸缩， 依托官方的ClusterAutoscaler组件，它可以根据部署的应用所请求的资源量来动态的伸缩集群，不同云厂商也分别实现了各自的Provider接入，对应的容器集群产品也都提供了弹性伸缩的功能，大家自行开启即可。如果你用的是自建集群， 可以参考云商实现适配自己集群的Provider。 在集群中部署CA主要解决以下几个问题： 1 思考需要多大的集群节点规模满足应用需求？ 2 当Pod Pending ，没有可调度节点？ 3 当多台节点利用率低， 不能被自动释放？ 整体架构 Autoscaler：核心模块，负责整体扩缩容功能 Estimator：负责评估计算扩容 Simulator：负责模拟调度，计算缩容 Cloud Provider：抽象了CloudProvider及NodeGroup等相关接口，与云API交互 CA架构 什么时候扩？ 由于资源不足，pod调度失败，导致pod处于pending状态时，CA会评估扩容节点过程， 新拉起一台机器加入到集群中 扩容流程 什么时候缩？ 当Node的资源利用率较低时，且此Node上存在的POD都能被重新调度到其他节点， CA组件就会执行评估缩容节点过程， 将这台Node节点上的POD驱逐到其他节点上，则此台Node就可以被释放 。需要注意适配缩容过程时需要考虑对POD的优雅调度 ， Cordon[10]节点 + Drain[11] 方式 缩容流程 什么样的节点不会被CA删除 节点上有pod被PodDisruptionBudget控制器限制。 节点上有命名空间是kube-system的pods。 节点上的pod不是被控制器创建，例如不是被deployment, replica set, job, stateful set创建。 节点上有pod使用了本地存储 节点上pod驱逐后无处可去，即没有其他node能调度这个pod 节点有注解：”cluster-autoscaler.kubernetes.io/scale-down-disabled”: “true” … ","date":"2022-08-01","objectID":"/%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E5%92%8C%E4%BC%98%E5%8C%96%E5%BB%BA%E8%AE%BE/:3:2","tags":["kubernetes"],"title":"[转载]企业级弹性伸缩和优化建设","uri":"/%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E5%92%8C%E4%BC%98%E5%8C%96%E5%BB%BA%E8%AE%BE/"},{"categories":["kubernetes"],"content":"3.3、Node节点超卖 Node 资源超卖方案是针对 Node 级别的资源调整方案，通过AdmissionWebhook[13]修改K8s Node的Allocatable 实现， 达到在使用率较低的情况下，将原本较小的资源扩展为逻辑上较大的资源来处理。例如原有CPU为4核，超卖2倍则是可将这个CPU视作8核 Node Annotation xxx/cpu-oversold: “false” 开启\u0026关闭 Node 超卖增加灵活性 Node Annotation xxx/cpu-oversold-ratio=1.5 调整Node可分配值到1.5倍 超卖功能主要解决以下几个问题： 1 应用资源一般申请都超标、真实负载不高 2 测试环境节点超卖2-3倍节省成本 整体架构 Node超卖架构 静态超卖 静态超卖就是不考虑节点的真实负载， 只通过注解配置资源超卖比例 动态超卖 动态超卖就是根据每个节点的真实负载数据，进行不同比例的资源超卖或取消超卖 自研组件基于节点历史监控数据，动态的/周期性的去调整超卖比例。比如某个 Node 连续 5m/1d持续低负载并且节点已分配资源水位线很高了，那么可以把超卖比例适当调高，以此使 Node 能容纳更多的业务 Pod。如果节点已经是高负载， 那就维持或调小超卖比 当然超卖的设计也有常见一些问题需要考虑，譬如需要不断平衡Node节点的超卖与真实使用率、要支持指标采集灵活性支持1m或更长1d的不同选择、考虑大规模集群（5k节点）频繁的webhook更新对调度的性能影响等 ","date":"2022-08-01","objectID":"/%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E5%92%8C%E4%BC%98%E5%8C%96%E5%BB%BA%E8%AE%BE/:3:3","tags":["kubernetes"],"title":"[转载]企业级弹性伸缩和优化建设","uri":"/%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E5%92%8C%E4%BC%98%E5%8C%96%E5%BB%BA%E8%AE%BE/"},{"categories":["kubernetes"],"content":"3.4、集群调度 在使用Node节点超卖时， 我们其实还有一个动态调度器配合一起工作， 调度器感知超卖相关配置和资源实际使用情况， 为应用稳定性和高效调度提供保证， 所以介绍下在调度层面可做的优化 静态调度 Kubernetes的资源编排调度默认使用的是静态调度Schedule[14]，就是通过调度中预选+优选方式将Pod与Node绑定。静态调度的好处就是调度简单高效、集群资源管理方便，最大的缺点也很明显，就是不管节点实际负载，极容易导致集群负载不高 动态调度 目前主要有3种扩展调度器方法： extender扩展调度[15]，包含预选和优选接口的webhook，自实现逻辑对节点进行过滤和打分，参与调度流程中各个阶段的决策 自定义调度器[16]，通过修改pod的spec.schedulerName来选择调度器。比较灵活，但研发成本高。当集群有默认调度器和自定义调度器时，会出现两个调度器争抢 Scheduling Framework[17] Kubernetes v1.19版本stable的可拔插的调度框架，开发者可以通过实现扩展点定义的接口来实现插件 —推荐 因主版本受限， 我们内部还是使用的extender， 通过实现 Filter webhook 接口，接收 scheduler 请求 Filter 基于服务画像（服务、节点的cpu、mem均值、方差、欧氏距离聚类算法）来优化调度 基本解决以下几个问题： 1 分配高负载低的情况、分配低但实际负载高的情况 2 节点CPU高负载75%以上不进行调度 3 机器打分制， 譬如ess机器分数最低 4 在线离线混部资源分配限制 开启自定义调度前 开启自定义调度后 ","date":"2022-08-01","objectID":"/%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E5%92%8C%E4%BC%98%E5%8C%96%E5%BB%BA%E8%AE%BE/:3:4","tags":["kubernetes"],"title":"[转载]企业级弹性伸缩和优化建设","uri":"/%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E5%92%8C%E4%BC%98%E5%8C%96%E5%BB%BA%E8%AE%BE/"},{"categories":["kubernetes"],"content":"3.5、Descheduler重调度 上面讲到kube-scheduler默认是静态调度，属于’一次性’调度， 因为 Pod一旦被绑定了节点是不会自动触发重新调度的，那在后续node节点数量、标签、污点、容忍等的变动可能会导致已经被调度过的pod不是最优调度， 官方descheduler[18] 组件补足了这一点。Descheduler可以根据一些规则和配置策略来帮助我们重新平衡集群状态，其核心原理是根据其策略配置找到可以被移除的 Pod 并驱逐它们，其本身并不会进行调度被驱逐的 Pod，而是依靠默认的调度器来实现，目前支持的策略有： RemoveDuplicates —在集群中打散 Pod LowNodeUtilization — 调度POD到未充分利用的节点上 RemovePodsViolatingInterPodAntiAffinity — 确保从节点中删除违反 Pod 反亲和性的 Pod RemovePodsViolatingNodeAffinity — 确保从节点中删除违反NoSchedule污点的 Pod RemovePodsViolatingNodeTaints — 确保从节点中删除违反节点亲和性的 Pod RemovePodsViolatingTopologySpreadConstraint — 确保从节点驱逐违反拓扑分布约束的 Pods RemovePodsHavingTooManyRestarts — 确保从节点中删除重启次数过多的 Pods PodLifeTime — 驱逐比maxPodLifeTimeSeconds更旧的 Pods ","date":"2022-08-01","objectID":"/%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E5%92%8C%E4%BC%98%E5%8C%96%E5%BB%BA%E8%AE%BE/:3:5","tags":["kubernetes"],"title":"[转载]企业级弹性伸缩和优化建设","uri":"/%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E5%92%8C%E4%BC%98%E5%8C%96%E5%BB%BA%E8%AE%BE/"},{"categories":["kubernetes"],"content":"3.6、离在线业务混部 因为离线和在线业务有明显的潮汐性，进行合理的混部提升资源利用率、降低成本的有效方案 。 在线服务：运行时间长，服务流量及资源利用率有潮汐特征，时延敏感，对服务 SLA 要求极高，如消息流 Feed 服务、电商交易服务等，一般在线 k8s 集群的高峰为 08:00 - 24:00 离线作业：运行时间分区间，运行期间资源利用率较高，时延不敏感，容错率高，中断一般允许重运行，如 Hadoop[21] 生态下的 MapReduce、Spark 作业，一般离线集群的高峰为 00:00 - 8:00 离线和在线业务混部是弹性的深水区， 因为不仅涉及调度部署、资源隔离与压制、容灾治理、跨部门合作等，所以一般也就只有大厂才真实落地 我介绍下我们在践行混部的路线，混部涉及到的比较复杂， 这里只介绍个思路， 大家也可以借鉴开源方案， 譬如阿里Koordinator[19]混部系统，腾讯的Caelus[20]混部方案等 整体架构 混部三步走 阶段1 实现 Hadoop 离线集群的资源共享 容器集群通过调度特定类型的服务特定阶段调度至 Hadoop 集群中空余的资源上，在离线集群的高峰到来之前进行迁移，保证离线集群高峰时期的资源 阶段2 整机腾挪方案 以集群转让节点的方式提高整体资源利用率 容器集群通过修改调度算法，实现低峰时期的空闲资源通过整机出让的方式，把机器资源共享至 Hadoop 离线集群 反之当在线服务的波峰来临，将Hadoop离线集群的机器通过整机出让的方式，把机器资源共享至 k8s在线集群 阶段3 离在线服务容器化混部 通过调度与隔离的手段进行资源共享与竞争 保障不同业务优先级(在线服务高优先级，离线服务低优先级) 离线服务低优先级，自动占用在线服务剩余宿主机资源 在线服务业务量增加的时候自动驱逐离线服务 ","date":"2022-08-01","objectID":"/%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E5%92%8C%E4%BC%98%E5%8C%96%E5%BB%BA%E8%AE%BE/:3:6","tags":["kubernetes"],"title":"[转载]企业级弹性伸缩和优化建设","uri":"/%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E5%92%8C%E4%BC%98%E5%8C%96%E5%BB%BA%E8%AE%BE/"},{"categories":["kubernetes"],"content":"3.7、Spot抢占实例 前文我们都是在介绍集群层面的技术优化，这一节介绍的是Spot资源。Spot抢占实例的资源成本大概只占按量实例的30%左右 Amazon EC2 Spot 实例让您可以利用 AWS 云中未使用的 EC2 容量。与按需实例的价格相比，使用 Spot 实例最高可以享受 90% 的折扣。得益于 AWS 的运行规模，Spot 实例可以在运行超大规模工作负载时实现规模并节省成本。您还可以选择休眠、停止或终止 Spot 实例，只需提前两分钟通知，我们就会收回 EC2 容量。 我们得到两个重要的信息， 1 spot实例最高可以享受90% 的折扣 2 实例终止提前两分钟通知 (阿里云是5分钟) 。那如果能用好抢占实例，就可以大幅降低我们的成本，所以我们尝试将云上业务的流量承载方式设置为如下： **30%**的预留实例用来承载基础流量 **10%**的按需实例用来做弹性按需 **60%**的抢占实例做弹性支撑 因为Spot实例具有随时会被回收终止的特点，所以Spot实例比较适合灵活性较高或具有容错性的应用程序，所以在K8s的无状态业务负载节点我们大量使用了Spot实例。如果你用的是云商的容器集群，那云商会提供Spot节点得选项， 如果是自建集群， 那建议通过阶段一、二完成建设 整体架构 Kubernetes+Spot抢占架构 阶段1 已AWS为例，通过SpotFleet[25]或ASG[26]获取所需的spot实例并加入k8s集群、通过CloudWatch[27]事件通知获取机器回收的消息、通过云函数Lambda[28]触发回收节点操作、完成spot实例优雅释放 阶段2 如果我们的业务达到很大的规模，建议通过离线训练、容器集群、优雅回收三个方向完整建设 首先通过对云商spot规格、价格、中断事件的数据收集和数据分析，考虑业务集群画像因素， 做深度智能推荐、中断预测 基于成本和稳定性双重考量，深度利用ondemand和spot实例组合，构建多可用区、业务分散策略、异常处理策略等 在节点工作阶段，通过中断预测提前预测实例回收事件发生概率，主动释放spot实例， 填充新的Node节点以减少不确定性，在节点回收阶段， 通过譬如提前扩容、弹性ECI兜底等减少pod不可调度风险 ","date":"2022-08-01","objectID":"/%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E5%92%8C%E4%BC%98%E5%8C%96%E5%BB%BA%E8%AE%BE/:3:7","tags":["kubernetes"],"title":"[转载]企业级弹性伸缩和优化建设","uri":"/%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E5%92%8C%E4%BC%98%E5%8C%96%E5%BB%BA%E8%AE%BE/"},{"categories":["kubernetes"],"content":"spot用法可以参考 容器成本降低50%，携程在AWS Spot上的实践[22] 趣头条[23] SpotIO[24] ","date":"2022-08-01","objectID":"/%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E5%92%8C%E4%BC%98%E5%8C%96%E5%BB%BA%E8%AE%BE/:4:0","tags":["kubernetes"],"title":"[转载]企业级弹性伸缩和优化建设","uri":"/%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E5%92%8C%E4%BC%98%E5%8C%96%E5%BB%BA%E8%AE%BE/"},{"categories":["kubernetes"],"content":"四 容器云应用弹性方向建设 前文提到kunernetes autoscaling 包括Pod 级别的自动伸缩和Node级别的自动伸缩， 接下来就聊聊Pod 级别的自动伸缩，基本包括： 原生的水平伸缩HPA[29] 事件驱动弹性KEDA[30] HPA的扩展-CronHPA、EHPA[31]、AHPA[32] 原生纵向伸缩Vertical Pod Autoscaler[9] VPA扩展-CronVPA Pod request超卖 ","date":"2022-08-01","objectID":"/%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E5%92%8C%E4%BC%98%E5%8C%96%E5%BB%BA%E8%AE%BE/:5:0","tags":["kubernetes"],"title":"[转载]企业级弹性伸缩和优化建设","uri":"/%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E5%92%8C%E4%BC%98%E5%8C%96%E5%BB%BA%E8%AE%BE/"},{"categories":["kubernetes"],"content":"4.1、水平伸缩HPA HPA[29] 会基于 kubernetes 集群内置资源指标或者自定义指标来计算 Pod 副本需求数并自动伸缩 ，因为伸缩的是副本数， 所以比较适合无状态应用deployment。kubernetes HPA 原生支持依据 CPU/Memory 资源利用率弹性伸缩，仅仅通过简单的命令就可以创建出hpa伸缩对象 kubectl autoscale deployment hpa-demo –cpu-percent=10 –min=2 –max=5 整体架构 HPA架构 Prometheus-adapter 在 HPA 实践落地过程中，仅仅依赖 CPU/Memory 利用率弹性伸缩无法满足业务在多指标扩缩、弹性伸缩稳定性方面的诸多需求，可以配置自定义的监控指标来，譬如 Prometheus-adapter[33] 1.如果metrics数据pod可以暴露，则hpa的metrics type可以为pod类型，可以创建servicemonitor[34]从pod采集监控上报prometheus，也可以pod直接上报给prometheus 2.如果metrics数据来源为k8s集群外部，比如ecs上的各种exporter，可以用servicemonitor从外部采集数据上报给集群内的prometheus,hpa的metrics type为object类型 HPA接入Prometheus-adapter 4.2、KEDA 我们看到原生HPA在支持多方数据指标时，需要做大量的工作来适配，并且原生HPA不支持预定的伸缩（解决弹性滞后的问题）、缩容到0（解决测试环境资源浪费问题）等问题，所以就出现各种基于HPA的扩展， 譬如当前最广泛应用的扩展器KEDA[30] KEDA是用于 Kubernetes 的基于事件驱动弹性伸缩器，用户只需要创建 ScaledObject 或 ScaledJob 来定义你想要伸缩的对象和你想要使用的触发器，KEDA 会处理剩下的一切！ 附上KEDA架构图和对比原生HPA接入 Prometheus adapter的对比图 KEDA架构 HPA接入外部指标架构 KEDA接入外部指标架构 4.3、CronHPA、EHPA CronHPA 工作的核心是基于检测到的周期做“定时规划”，通过规划实现提前扩容的目的 EHPA[31]、AHPA[32] 基于预测算法，提前触发应用扩容，大家感兴趣的可以深入研究下腾讯开源的基于FinOps为核心理念的Crane[31]项目 EHPA架构 ","date":"2022-08-01","objectID":"/%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E5%92%8C%E4%BC%98%E5%8C%96%E5%BB%BA%E8%AE%BE/:5:1","tags":["kubernetes"],"title":"[转载]企业级弹性伸缩和优化建设","uri":"/%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E5%92%8C%E4%BC%98%E5%8C%96%E5%BB%BA%E8%AE%BE/"},{"categories":["kubernetes"],"content":"4.4、垂直扩容VPA Vertical Pod Autoscaler[9]是垂直扩容，根据 Pod 的资源利用率、历史数据、异常事件，来动态调整负载的 Request 值的组件，主要关注在有状态服务、单体应用的资源伸缩场景 整体架构 该项目包括3个组成部分： Recommender - 它监视当前和过去的资源消耗，并根据它提供推荐值容器的CPU和内存请求。 Updater - 它检查哪些托管窗格具有正确的资源集，如果没有，则检查它们，以便控制器可以使用更新的请求重新创建它们 Admission Plugin - 它在新pod上设置正确的资源请求（由于Updater的活动而刚刚由其控制器创建或重新创建） VPA架构 不过VPA也有一些限制条件： VPA更新资源时要重建POD VPA和HPA同时工作时的冲突问题不好解决 VPA性能尚未在大型集群中进行测试 除了VPA， 还有没有其他方式可以对资源设置的request资源进行弹性优化呢， 这里也介绍另外两种方式 4.5、CronVPA 基本的核心还是基于检测到的周期做“定时规划”，根据业务的过去的监控数据，规划实现提前修改request的目的， 虽然也避免不了POD要进行重建 4.6、POD Request 超卖 前文我们介绍过Node节点超卖，那还有一种更细粒度的就是Pod 的资源超卖，实现方式一样是通过webhook机制动态更改Pod的request设置（limit不超卖），或者更简单粗暴一些我们在研发申请资源的时候按照静态超卖的配置比例调整request设定， 当然这里要评估对业务稳定性的影响 ","date":"2022-08-01","objectID":"/%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E5%92%8C%E4%BC%98%E5%8C%96%E5%BB%BA%E8%AE%BE/:5:2","tags":["kubernetes"],"title":"[转载]企业级弹性伸缩和优化建设","uri":"/%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E5%92%8C%E4%BC%98%E5%8C%96%E5%BB%BA%E8%AE%BE/"},{"categories":["kubernetes"],"content":"五 基于混合云弹性建设 从弹性的视角来讲私有云平台可以应对常态化业务访问压力，那如果遇到流量剧增，公有云更能提供成熟可靠的秒级弹性伸缩服，因此进行混合云建设可以有效填补业务高低峰访问对资源需求波动较大的业务需求场景，与私有云平台实现优势互补；当然混合云的建设不仅是为弹性，更多是为议价、多活、容灾考虑。如下我以私有云IDC和阿里云为例，举例其中一种混合云的形态，数据层是DTS[35]同步、流量通过云解析DNS实现按照权重分发： 混合云架构 ","date":"2022-08-01","objectID":"/%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E5%92%8C%E4%BC%98%E5%8C%96%E5%BB%BA%E8%AE%BE/:6:0","tags":["kubernetes"],"title":"[转载]企业级弹性伸缩和优化建设","uri":"/%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E5%92%8C%E4%BC%98%E5%8C%96%E5%BB%BA%E8%AE%BE/"},{"categories":["kubernetes"],"content":"六 Serverless 文中提到的这些资源伸缩及优化的方式，无疑都需要有个强大的技术团队来做支撑，并且这些对未来的大多数公司来讲也不是弹性优化的首选，或许Serverless才是云原生弹性演进的未来 Serverless 不是特指某种技术，而是无服务器、按需弹性、按量计费、简化运维等的工具结合， 通过一个应用引擎框架集成云开发、FaaS、BaaS、托管K8s、DevOps、服务治理、弹性等帮助业务轻松上云 比较流行的的开源框架像Serverless[36] Knative[37] OpenFass[38] Serverless架构 ","date":"2022-08-01","objectID":"/%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E5%92%8C%E4%BC%98%E5%8C%96%E5%BB%BA%E8%AE%BE/:7:0","tags":["kubernetes"],"title":"[转载]企业级弹性伸缩和优化建设","uri":"/%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E5%92%8C%E4%BC%98%E5%8C%96%E5%BB%BA%E8%AE%BE/"},{"categories":["kubernetes"],"content":"七 总结 本文介绍了企业能够实施和采纳的弹性伸缩及优化的建设路线，基于上述部分方向建设，我们成功将资源CPU平均利用率从10%提升到30%，云服务器月度成本降低60%。本文更多是以全局视角让大家理解在弹性伸缩及优化建设的方向，希望对大家有所帮助。并未介绍过多技术细节，后面有机会再对文中方案做细节介绍 ","date":"2022-08-01","objectID":"/%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E5%92%8C%E4%BC%98%E5%8C%96%E5%BB%BA%E8%AE%BE/:8:0","tags":["kubernetes"],"title":"[转载]企业级弹性伸缩和优化建设","uri":"/%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E5%92%8C%E4%BC%98%E5%8C%96%E5%BB%BA%E8%AE%BE/"},{"categories":["kubernetes"],"content":"参考资料 ","date":"2022-08-01","objectID":"/%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E5%92%8C%E4%BC%98%E5%8C%96%E5%BB%BA%E8%AE%BE/:8:1","tags":["kubernetes"],"title":"[转载]企业级弹性伸缩和优化建设","uri":"/%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E5%92%8C%E4%BC%98%E5%8C%96%E5%BB%BA%E8%AE%BE/"},{"categories":["kubernetes"],"content":"[1]: https://kubernetes.io/ kubernetes [2]: https://en.wikipedia.org/wiki/Serverless_computing “Serverless Wiki” [3]: https://kubernetes.io/docs/concepts/architecture/nodes/ “Node” [4]: https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework/ “自定义调度” [5]: https://github.com/kubernetes-sigs/descheduler “重调度” [6]: https://aws.amazon.com/ec2/spot/?nc1=h_ls “Aws Spot” [7]: https://github.com/kubernetes/autoscaler “autoscaler” [8]: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/ “HPA” [9]: https://github.com/kubernetes/design-proposals-archive/blob/main/autoscaling/vertical-pod-autoscaler.md “VPA” [10]: https://kubernetes.io/zh-cn/docs/concepts/architecture/nodes/ “Cordon” [11]: https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/ “Drain” [12]: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/ “PDB” [13]: https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/ “Dynamic Admission Control” [14]: https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/ “Scheduler” [15]: https://kubernetes.io/docs/concepts/extend-kubernetes/#scheduler-extensions “扩展调度” [16]: https://kubernetes.io/docs/tasks/extend-kubernetes/configure-multiple-schedulers/ “多调度器” [17]: https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework/ “scheduling-framework” [18]: https://github.com/kubernetes-sigs/descheduler “重调度” [19]: https://github.com/koordinator-sh/koordinator “koordinator” [20]: https://github.com/Tencent/Caelus “Caelus” [21]: https://hadoop.apache.org/ “hadoop” [22]: https://mp.weixin.qq.com/s/xqsNeN28TCS5YzesBUIsxw “干货 | 容器成本降低50%，携程在AWS Spot上的实践” [23]: http://cloud.qutoutiao.net/ “趣头条” [24]: https://spot.io/ “spotio” [25]: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-fleet.html “spot-fleet” [26]: https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-groups.html “ASG” [27]: https://aws.amazon.com/cloudwatch/ “cloudwatch” [28]: https://aws.amazon.com/lambda/ “lambda” [29]: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/ “HPA” [30]: https://keda.sh/ “KEDA” [31]: https://github.com/gocrane/crane “EHPA” [32]: https://www.alibabacloud.com/blog/599120 “AHPA” [33]: https://github.com/kubernetes-sigs/prometheus-adapter “prometheus-adapter” [34]: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/getting-started.md “prometheus-monitor” [35]: https://help.aliyun.com/product/26590.html “阿里云DTS” [36]: https://www.serverless.com/ “serverless” [37]: https://knative.dev/ “knative” [38]: https://www.openfaas.com/ “openfaas” [39]: https://cloud.tencent.com/developer/article/1505214 “腾讯自研业务上云：优化Kubernetes集群负载的技术方案探讨” [40]: https://mp.weixin.qq.com/s/KCHlbyNyJyOuZ9WJyx1qxg Vivo计算平台弹性实践 ","date":"2022-08-01","objectID":"/%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E5%92%8C%E4%BC%98%E5%8C%96%E5%BB%BA%E8%AE%BE/:8:2","tags":["kubernetes"],"title":"[转载]企业级弹性伸缩和优化建设","uri":"/%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E5%92%8C%E4%BC%98%E5%8C%96%E5%BB%BA%E8%AE%BE/"},{"categories":null,"content":"Ingress 资源是 Kubernetes 众多成功故事之一。它创建了一个不同的 Ingress 控制器生态系统，这些控制器以标准化和一致的方式在成千上万的集群中使用。这种标准化帮助用户采用 Kubernetes。然而，在 Ingress 创建 5 年后，有迹象表明，分裂为不同但惊人相似的 CRD 和超载的注释。使 Ingress 普及的可移植性同样也限制了它的未来。 在 2019 年圣地亚哥 Kubecon 大会上，一群热情的贡献者聚集在一起讨论 Ingress 的演变。讨论蔓延到了街对面的酒店大厅，结果就是后来被称为 Gateway API 的东西。这一讨论是基于以下几个关键假设： 作为路由匹配、流量管理和服务暴露基础的 API 标准已经商品化，作为自定义 API 对其实现者和用户几乎没有提供什么价值 可以通过共同的核心 API 资源来表示 L4/L7 路由和流量管理 以一种不牺牲核心 API 的用户体验的方式，为更复杂的功能提供可扩展性是可能的 ","date":"2022-07-29","objectID":"/gateway-api/:0:0","tags":null,"title":"通过Gateway API不断演变的Kubernetes网络","uri":"/gateway-api/"},{"categories":null,"content":"引入 Gateway API 这就引出了允许 Gateway API 在 Ingress 基础上改进的设计原则： 表达能力——除了 HTTP 主机/路径匹配和 TLS 之外，Gateway API 还可以表达 HTTP 头操作、流量加权和镜像、TCP/UDP 路由以及其他只能在 Ingress 中通过自定义注释才能实现的功能。 面向角色的设计——API 资源模型反映了在路由和 Kubernetes 服务网络中常见的职责分离。 可扩展性——资源允许在 API 的不同层上附加任意的配置。这使得在最合适的地方可以进行细粒度定制。 灵活的一致性——Gateway API 定义了不同的一致性级别——核心（强制支持）、扩展（如果支持则可移植）和自定义（没有可移植性保证），统称为灵活的一致性[1]。这促进了一个高度可移植的核心 API（如 Ingress），它仍然为网关控制器实现者提供灵活性。 ","date":"2022-07-29","objectID":"/gateway-api/:1:0","tags":null,"title":"通过Gateway API不断演变的Kubernetes网络","uri":"/gateway-api/"},{"categories":null,"content":"Gateway API 是什么样子的？ Gateway API 引入了一些新的资源类型： GatewayClasses 是集群范围的资源，作为模板来显式定义从它们派生的 Gateways 的行为。这在概念上类似于 StorageClasses，但用于联网数据平面。 Gateways 是 GatewayClasses 的部署实例。它们是执行路由的数据平面的逻辑表示，可以是集群内代理、硬件 LB 或云 LB。 路由不是单一的资源，而是代表许多不同协议特定的 Route 资源。HTTPRoute 具有匹配、过滤和路由规则，这些规则应用于能够处理 HTTP 和 HTTPS 流量的 Gateways。类似地，TCPRoutes、UDPRoutes 和 TLSRoutes 也具有特定于协议的语义。此模型还允许 Gateway API 将来增量地扩展其协议支持。 ","date":"2022-07-29","objectID":"/gateway-api/:1:1","tags":null,"title":"通过Gateway API不断演变的Kubernetes网络","uri":"/gateway-api/"},{"categories":null,"content":"网关控制器实现 好消息是，尽管 Gateway 是在Alpha[2]阶段，但已经有几个你可以运行的Gateway 控制器实现[3]。由于它是一个标准化的规范，下面的示例可以在它们中的任何一个上运行，并且应该以完全相同的方式工作。查看入门手册[4]，了解如何安装和使用这些网关控制器之一。 ","date":"2022-07-29","objectID":"/gateway-api/:1:2","tags":null,"title":"通过Gateway API不断演变的Kubernetes网络","uri":"/gateway-api/"},{"categories":null,"content":"Gateway API 例子 在下面的例子中，我们将演示不同 API 资源之间的关系，并带你浏览一个常见的用例： 团队 foo 将他们的应用部署在 foo 命名空间中。他们需要控制应用程序不同页面的路由逻辑。 团队 bar 运行在 bar 命名空间中。他们希望能够对他们的应用进行蓝绿发布以降低风险。 平台团队负责管理 Kubernetes 集群中所有应用的负载均衡器和网络安全。 下面的 foo-route 对 foo 命名空间中的各种服务进行路径匹配，并且还有一个到 404 服务器的默认路由。这将分别通过 foo.example.com/login 和 foo.example.com/home 暴露 foo-auth 和 foo-home 服务： kind: HTTPRoute apiVersion: networking.x-k8s.io/v1alpha1 metadata: name: foo-route namespace: foo labels: gateway: external-https-prod spec: hostnames: - \"foo.example.com\" rules: - matches: - path: type: Prefix value: /login forwardTo: - serviceName: foo-auth port: 8080 - matches: - path: type: Prefix value: /home forwardTo: - serviceName: foo-home port: 8080 - matches: - path: type: Prefix value: / forwardTo: - serviceName: foo-404 port: 8080 bar 团队在同一个 Kubernetes 集群的 bar 命名空间中操作，也希望将他们的应用程序暴露给互联网，但他们也希望控制自己的灰度和蓝绿发布。下面的 HTTPRoute 被配置为以下行为： bar.example.com 的流量： 将 90%的流量发送到 bar-v1 将 10%的流量发送给 bar-v2 对于使用 HTTP 头 env:canary 到 bar.example.com 的流量： 将所有流量发送到 bar-v2 kind: HTTPRoute apiVersion: networking.x-k8s.io/v1alpha1 metadata: name: bar-route namespace: bar labels: gateway: external-https-prod spec: hostnames: - \"bar.example.com\" rules: - forwardTo: - serviceName: bar-v1 port: 8080 weight: 90 - serviceName: bar-v2 port: 8080 weight: 10 - matches: - headers: values: env: canary forwardTo: - serviceName: bar-v2 port: 8080 ","date":"2022-07-29","objectID":"/gateway-api/:2:0","tags":null,"title":"通过Gateway API不断演变的Kubernetes网络","uri":"/gateway-api/"},{"categories":null,"content":"路由和网关绑定 因此，我们有两个匹配的 HTTPRoutes，并将流量路由到不同的服务。你可能想知道，在哪里可以访问这些服务？它们通过哪些网络或 IP 暴露？ 路由如何向客户端暴露由路由绑定[5]来管理，该绑定描述了路由和网关之间如何创建双向关系。当 Routes 被绑定到一个 Gateway 时，这意味着它们的集合路由规则被配置在底层的负载均衡器或代理上，并且路由可以通过网关访问。因此，网关是可以通过路由配置的网络数据平面的逻辑表示。 ","date":"2022-07-29","objectID":"/gateway-api/:2:1","tags":null,"title":"通过Gateway API不断演变的Kubernetes网络","uri":"/gateway-api/"},{"categories":null,"content":"行政委托 Gateway 和 Route 资源之间的分离允许集群管理员将一些路由配置委派给各个团队，同时仍然保持集中控制。以下网关资源在端口 443 上暴露 HTTPS，并使用由集群管理员控制的证书终止端口上的所有通信流。 kind: Gateway apiVersion: networking.x-k8s.io/v1alpha1 metadata: name: prod-web spec: gatewayClassName: acme-lb listeners: - protocol: HTTPS port: 443 routes: kind: HTTPRoute selector: matchLabels: gateway: external-https-prod namespaces: from: All tls: certificateRef: name: admin-controlled-cert 下面的 HTTPRoute 展示了 Route 如何通过它的 kind（HTTPRoute）和资源标签（gateway=external-https-prod）来确保它匹配网关的选择器。 # Matches the required kind selector on the Gateway kind: HTTPRoute apiVersion: networking.x-k8s.io/v1alpha1 metadata: name: foo-route namespace: foo-ns labels: # Matches the required label selector on the Gateway gateway: external-https-prod ... ","date":"2022-07-29","objectID":"/gateway-api/:2:2","tags":null,"title":"通过Gateway API不断演变的Kubernetes网络","uri":"/gateway-api/"},{"categories":null,"content":"面向角色的设计 当你将它们放在一起时，你就拥有了一个可以被多个团队安全地共享的负载平衡基础设施。Gateway API 不仅是用于高级路由的更具表现力的 API，而且是面向角色的 API，专为多租户基础设施设计。它的可扩展性确保了它将在保持可移植性的同时为未来的用例发展。最终，这些特性将允许 Gateway API 适应不同的组织模型和实现，直到未来。 ","date":"2022-07-29","objectID":"/gateway-api/:2:3","tags":null,"title":"通过Gateway API不断演变的Kubernetes网络","uri":"/gateway-api/"},{"categories":null,"content":"尝试一下，并参与其中 有许多资源可以查看以了解更多。 查看入门手册，看看可以解决哪些用例。 尝试使用现有的网关控制器之一 或者参与[6]并帮助设计和影响 Kubernetes 服务网络的未来！ ","date":"2022-07-29","objectID":"/gateway-api/:2:4","tags":null,"title":"通过Gateway API不断演变的Kubernetes网络","uri":"/gateway-api/"},{"categories":null,"content":"参考资料 [1] 灵活的一致性: https://gateway-api.sigs.k8s.io/concepts/guidelines/#conformance [2] Alpha: https://github.com/kubernetes-sigs/gateway-api/releases [3] Gateway 控制器实现: https://gateway-api.sigs.k8s.io/references/implementations/ [4] 入门手册: https://gateway-api.sigs.k8s.io/guides/getting-started/ [5] 路由绑定: https://gateway-api.sigs.k8s.io/concepts/api-overview/#route-binding [6] 参与: https://gateway-api.sigs.k8s.io/contributing/community/ ","date":"2022-07-29","objectID":"/gateway-api/:2:5","tags":null,"title":"通过Gateway API不断演变的Kubernetes网络","uri":"/gateway-api/"},{"categories":["Operator"],"content":"安装kubebuilder brew install kubebuilder kubebuilder version ","date":"2022-07-21","objectID":"/kubebuilder%E5%BC%80%E5%8F%91operator/:0:1","tags":["kubebuilder","Operator"],"title":"Kubebuilder开发简单的Operator","uri":"/kubebuilder%E5%BC%80%E5%8F%91operator/"},{"categories":["Operator"],"content":"创建项目目录 mkdir custom-controllers cd custom-controllers go mod init controllers.alongparty.cn ","date":"2022-07-21","objectID":"/kubebuilder%E5%BC%80%E5%8F%91operator/:0:2","tags":["kubebuilder","Operator"],"title":"Kubebuilder开发简单的Operator","uri":"/kubebuilder%E5%BC%80%E5%8F%91operator/"},{"categories":["Operator"],"content":"kubebuilder初始化项目 kubebuilder init --domain controller.alongparty.cn --license apache2 --owner \"kbsonlong\" kubebuilder create api --group controller --version v1 --kind Application make ","date":"2022-07-21","objectID":"/kubebuilder%E5%BC%80%E5%8F%91operator/:0:3","tags":["kubebuilder","Operator"],"title":"Kubebuilder开发简单的Operator","uri":"/kubebuilder%E5%BC%80%E5%8F%91operator/"},{"categories":["Operator"],"content":"参考资料 使用kubebuilder开发operator详解 ","date":"2022-07-21","objectID":"/kubebuilder%E5%BC%80%E5%8F%91operator/:0:4","tags":["kubebuilder","Operator"],"title":"Kubebuilder开发简单的Operator","uri":"/kubebuilder%E5%BC%80%E5%8F%91operator/"},{"categories":["Operator"],"content":"准备工作 S2I自定义镜像构建器 ● assemble（必需）：从源代码构建应用程序制品的脚本 assemble。 ● run（必需）：用于运行应用程序的脚本。 ● save-artifacts（可选）：管理增量构建过程中的所有依赖。 ● usage（可选）：提供说明的脚本。 ● test （可选）：用于测试的脚本。 ","date":"2022-07-21","objectID":"/s2i%E8%87%AA%E5%AE%9A%E4%B9%89%E6%9E%84%E5%BB%BA%E5%99%A8%E5%92%8C%E6%A8%A1%E6%9D%BF/:0:1","tags":["kubebuilder","Operator"],"title":"Kubesphere S2I自定义构建器和模板","uri":"/s2i%E8%87%AA%E5%AE%9A%E4%B9%89%E6%9E%84%E5%BB%BA%E5%99%A8%E5%92%8C%E6%A8%A1%E6%9D%BF/"},{"categories":["Operator"],"content":"创建镜像构建器 准备S2I目录 安装s2i wget https://github.com/openshift/source-to-image/releases/download/v1.2.04/source-to-image-v1.1.14-874754de-linux-386.tar.gz tar -xvf source-to-image-v1.1.14-874754de-linux-386.tar.gz ls s2i source-to-image-v1.1.14-874754de-linux-386.tar.gz sti cp s2i /usr/local/bin 初始化镜像构建器 s2i create java-ubuntu22 java-ubuntu22 cd java-ubuntu22 目录结构初始化 . ├── Dockerfile ├── Makefile ├── README.md ├── prometheus-config.yml ├── s2i │ └── bin │ ├── assemble │ ├── run │ ├── save-artifacts │ └── usage └── test ├── run └── test-app ├── b2i-jar-java8.jar ## 默认不存在 └── index.html 4 directories, 11 files ","date":"2022-07-21","objectID":"/s2i%E8%87%AA%E5%AE%9A%E4%B9%89%E6%9E%84%E5%BB%BA%E5%99%A8%E5%92%8C%E6%A8%A1%E6%9D%BF/:0:2","tags":["kubebuilder","Operator"],"title":"Kubesphere S2I自定义构建器和模板","uri":"/s2i%E8%87%AA%E5%AE%9A%E4%B9%89%E6%9E%84%E5%BB%BA%E5%99%A8%E5%92%8C%E6%A8%A1%E6%9D%BF/"},{"categories":["Operator"],"content":"修改Dockerfile # java-ubuntu22 FROM ubuntu:22.04 # TODO: Put the maintainer name in the image metadata # LABEL maintainer=\"Your Name \u003cyour@email.com\u003e\" # TODO: Rename the builder environment variable to inform users about application you provide them # ENV BUILDER_VERSION 1.0 # TODO: Set labels used in OpenShift to describe the builder image LABEL io.k8s.description=\"Java 8 web application\" \\ io.k8s.display-name=\"Java 8 Web\" \\ io.openshift.expose-services=\"8080:http\" \\ io.openshift.tags=\"builder,java,web\" \\ # this label tells s2i where to find its mandatory scripts # (run, assemble, save-artifacts) io.openshift.s2i.scripts-url=\"image:///usr/libexec/s2i\" # TODO: Install required packages here: # RUN yum install -y ... \u0026\u0026 yum clean all -y RUN apt-get update \u0026\u0026 apt-get install -y \\ curl \\ wget \\ openjdk-8-jdk \u0026\u0026 \\ rm -rf /var/lib/apt/lists WORKDIR /opt # TODO (optional): Copy the builder files into /opt/app-root # COPY ./\u003cbuilder_folder\u003e/ /opt/app-root/ # TODO: Copy the S2I scripts to /usr/libexec/s2i, since openshift/base-centos7 image # sets io.openshift.s2i.scripts-url label that way, or update that label COPY ./s2i/bin/ /usr/libexec/s2i # TODO: Drop the root user and make the content of /opt/app-root owned by user 1001 # RUN chown -R 1001:1001 /opt/app-root RUN chgrp -R 0 /usr/libexec/s2i \\ \u0026\u0026 chmod -R u=rwx,go=rx /usr/libexec/s2i \u0026\u0026 \\ chgrp -R 0 /opt \\ \u0026\u0026 chmod -R u=rwx,go=rx /opt \u0026\u0026 \\ chown -R 1001:1001 /opt \u0026\u0026 \\ chown -R 1001:1001 /usr/libexec/s2i/assemble RUN mkdir -p /opt/prometheus/etc \\ \u0026\u0026 curl https://repo1.maven.org/maven2/io/prometheus/jmx/jmx_prometheus_javaagent/0.17.0/jmx_prometheus_javaagent-0.17.0.jar \\ -o /opt/prometheus/jmx_prometheus_javaagent.jar COPY prometheus-config.yml /opt/prometheus/etc # This default user is created in the openshift/base-centos7 image USER 1001 # TODO: Set the default port for applications built using this image EXPOSE 8080 # TODO: Set the default CMD for the image CMD [\"/usr/libexec/s2i/usage\"] ","date":"2022-07-21","objectID":"/s2i%E8%87%AA%E5%AE%9A%E4%B9%89%E6%9E%84%E5%BB%BA%E5%99%A8%E5%92%8C%E6%A8%A1%E6%9D%BF/:0:3","tags":["kubebuilder","Operator"],"title":"Kubesphere S2I自定义构建器和模板","uri":"/s2i%E8%87%AA%E5%AE%9A%E4%B9%89%E6%9E%84%E5%BB%BA%E5%99%A8%E5%92%8C%E6%A8%A1%E6%9D%BF/"},{"categories":["Operator"],"content":"修改S2I脚本 assemble #!/bin/bash -e # # S2I assemble script for the 'java-ubuntu22' image. # The 'assemble' script builds your application source so that it is ready to run. # # For more information refer to the documentation: # https://github.com/openshift/source-to-image/blob/master/docs/builder_image.md # # If the 'java-ubuntu22' assemble script is executed with the '-h' flag, print the usage. if [[ \"$1\" == \"-h\" ]]; then exec /usr/libexec/s2i/usage fi # Restore artifacts from the previous build (if they exist). # echo \"---\u003e Installing application...\" mkdir -p /opt/app ls /tmp/src/* mv /tmp/src/* /opt/app/ chmod +x /opt/app/*.jar 默认情况下，s2i build将应用程序源代码放在/tmp/src。上述命令将应用程序jar包复制到/opt/app/目录下 run #!/bin/bash -e # # S2I run script for the 'java-ubuntu22' image. # The run script executes the server that runs your application. # # For more information see the documentation: # https://github.com/openshift/source-to-image/blob/master/docs/builder_image.md # PROMETHEUS_JMX_OPTS=\"-javaagent:/opt/prometheus/jmx_prometheus_javaagent.jar=${PROMETHEUS_JMX_PORT}:/opt/prometheus/etc/prometheus-config.yml\" # Always include jolokia-opts, which can be empty if switched off via env JAVA_OPTIONS=\"${JAVA_OPTIONS:+${JAVA_OPTIONS} }\" # Temporary options variable until the harmonization hawt-app PR #5 has been applied (hopefully) JVM_ARGS=\"${JVM_ARGS:+${JVM_ARGS} }${JAVA_OPTIONS} ${PROMETHEUS_JMX_OPTS}\" export JAVA_OPTIONS JVM_ARGS PROMETHEUS_JMX_OPTS exec java -jar ${JVM_ARGS} /opt/app/app.jar ","date":"2022-07-21","objectID":"/s2i%E8%87%AA%E5%AE%9A%E4%B9%89%E6%9E%84%E5%BB%BA%E5%99%A8%E5%92%8C%E6%A8%A1%E6%9D%BF/:0:4","tags":["kubebuilder","Operator"],"title":"Kubesphere S2I自定义构建器和模板","uri":"/s2i%E8%87%AA%E5%AE%9A%E4%B9%89%E6%9E%84%E5%BB%BA%E5%99%A8%E5%92%8C%E6%A8%A1%E6%9D%BF/"},{"categories":["Operator"],"content":"构建与运行 创建镜像构建器 make build 使用镜像构建器创建应用程序镜像 # s2i build ./test/test-app java-ubuntu22 sample-app ---\u003e Installing application... /tmp/src/b2i-jar-java8.jar /tmp/src/index.html Build completed successfully 按照assemble脚本定义的逻辑，S2I使用镜像构建器作为基础创建应用程序镜像，并从test/test-app目录注入源代码。 测试运行应用程序镜像 # docker run -p 8080:8080 -p 12345:12345 -e PROMETHEUS_JMX_PORT=12345 sample-app . ____ _ __ _ _ /\\\\ / ___'_ __ _ _(_)_ __ __ _ \\ \\ \\ \\ ( ( )\\___ | '_ | '_| | '_ \\/ _` | \\ \\ \\ \\ \\\\/ ___)| |_)| | | | | || (_| | ) ) ) ) ' |____| .__|_| |_|_| |_\\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v1.4.1.BUILD-SNAPSHOT) 2022-07-28 07:51:48.829 INFO 1 --- [ main] io.kubesphere.devops.Application : Starting Application v0.0.1-SNAPSHOT on 322634c30cc2 with PID 1 (/opt/app/app.jar started by ? in /opt) 2022-07-28 07:51:48.839 INFO 1 --- [ main] io.kubesphere.devops.Application : No active profile set, falling back to default profiles: default 2022-07-28 07:51:48.906 INFO 1 --- [ main] ationConfigEmbeddedWebApplicationContext : Refreshing org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@5ae50ce6: startup date [Thu Jul 28 07:51:48 GMT 2022]; root of context hierarchy 2022-07-28 07:51:49.694 INFO 1 --- [ main] s.b.c.e.t.TomcatEmbeddedServletContainer : Tomcat initialized with port(s): 8080 (http) 2022-07-28 07:51:49.700 INFO 1 --- [ main] o.apache.catalina.core.StandardService : Starting service Tomcat 2022-07-28 07:51:49.701 INFO 1 --- [ main] org.apache.catalina.core.StandardEngine : Starting Servlet Engine: Apache Tomcat/8.5.5 2022-07-28 07:51:49.749 INFO 1 --- [ost-startStop-1] o.a.c.c.C.[Tomcat].[localhost].[/] : Initializing Spring embedded WebApplicationContext 2022-07-28 07:51:49.749 INFO 1 --- [ost-startStop-1] o.s.web.context.ContextLoader : Root WebApplicationContext: initialization completed in 845 ms 2022-07-28 07:51:49.820 INFO 1 --- [ost-startStop-1] o.s.b.w.servlet.ServletRegistrationBean : Mapping servlet: 'dispatcherServlet' to [/] 2022-07-28 07:51:49.822 INFO 1 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean : Mapping filter: 'characterEncodingFilter' to: [/*] 2022-07-28 07:51:49.822 INFO 1 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean : Mapping filter: 'hiddenHttpMethodFilter' to: [/*] 2022-07-28 07:51:49.823 INFO 1 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean : Mapping filter: 'httpPutFormContentFilter' to: [/*] 2022-07-28 07:51:49.823 INFO 1 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean : Mapping filter: 'requestContextFilter' to: [/*] 2022-07-28 07:51:49.988 INFO 1 --- [ main] s.w.s.m.m.a.RequestMappingHandlerAdapter : Looking for @ControllerAdvice: org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@5ae50ce6: startup date [Thu Jul 28 07:51:48 GMT 2022]; root of context hierarchy 2022-07-28 07:51:50.047 INFO 1 --- [ main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped \"{[/]}\" onto public java.lang.String io.kubesphere.devops.HelloWorldController.sayHello() 2022-07-28 07:51:50.050 INFO 1 --- [ main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped \"{[/error],produces=[text/html]}\" onto public org.springframework.web.servlet.ModelAndView org.springframework.boot.autoconfigure.web.BasicErrorController.errorHtml(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse) 2022-07-28 07:51:50.050 INFO 1 --- [ main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped \"{[/error]}\" onto public org.springframework.http.ResponseEntity\u003cjava.util.Map\u003cjava.lang.String, java.lang.Object\u003e\u003e org.springframework.boot.autoconfigure.web.BasicErrorController.error(javax.servlet.http.HttpServletRequest) 2022-07-28 07:51:50.063 INFO 1 --- [ main] o.s.w.s.handler.SimpleUrlHandlerMapping : Mapped URL path [/webjars/**] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler] 2022-07-28 07:51:50.063 INFO 1 --- [ main] o.s.w.s.handler.SimpleUrlHandlerMapping :","date":"2022-07-21","objectID":"/s2i%E8%87%AA%E5%AE%9A%E4%B9%89%E6%9E%84%E5%BB%BA%E5%99%A8%E5%92%8C%E6%A8%A1%E6%9D%BF/:0:5","tags":["kubebuilder","Operator"],"title":"Kubesphere S2I自定义构建器和模板","uri":"/s2i%E8%87%AA%E5%AE%9A%E4%B9%89%E6%9E%84%E5%BB%BA%E5%99%A8%E5%92%8C%E6%A8%A1%E6%9D%BF/"},{"categories":["Operator"],"content":"自定义S2I模板 apiVersion: devops.kubesphere.io/v1alpha1 kind: S2iBuilderTemplate metadata: labels: controller-tools.k8s.io: \"1.0\" builder-type.kubesphere.io/s2i: \"s2i\" name: java-ubuntu spec: containerInfo: - builderImage: java-ubuntu22 ## 自定义的镜像构建器镜像 codeFramework: java # type of code framework defaultBaseImage: java-ubuntu22 # default Image Builder (can be replaced by a customized image) version: 0.0.1 # Builder template version description: \"模板描述\" # Builder template description kubectl apply -f s2ibuildertemplate.yaml 标签名称 选项 定义 builder-type.kubesphere.io/s2i “s2i” 模板类型为 S2I，基于应用程序源代码构建镜像。 builder-type.kubesphere.io/b2i “b2i” 模板类型为 B2I，基于二进制文件或其他制品构建镜像。 binary-type.kubesphere.io “jar”,“war”,“binary” 该类型为 B2I 类型的补充，在选择 B2I 类型时需要。例如，当提供 Jar 包时，选择 “jar” 类型。在 KubeSphere v2.1.1 及更高版本，允许自定义 B2I 模板。 官方demo apiVersion: devops.kubesphere.io/v1alpha1 kind: S2iBuilderTemplate metadata: annotations: descriptionCN: Java 应用的构建器模版。通过该模版可构建出直接运行的应用镜像。 descriptionEN: This is a builder template for Java builds whose result can be run directly without any further application server.It's suited ideally for microservices with a flat classpath (including \"far jars\"). devops.kubesphere.io/s2i-template-url: https://github.com/kubesphere/s2i-java-container/blob/master/java/images helm.sh/hook: pre-install labels: binary-type.kubesphere.io: jar builder-type.kubesphere.io/b2i: b2i builder-type.kubesphere.io/s2i: s2i controller-tools.k8s.io: \"1.0\" name: java spec: codeFramework: java containerInfo: - buildVolumes: - s2i_java_cache:/tmp/artifacts builderImage: 969049650220.dkr.ecr.ap-east-1.amazonaws.com/kubesphere/java-8-centos7:v3.2.0 runtimeArtifacts: - source: /deployments runtimeImage: 969049650220.dkr.ecr.ap-east-1.amazonaws.com/kubesphere/java-8-runtime:v3.2.0 - buildVolumes: - s2i_java_cache:/tmp/artifacts builderImage: 969049650220.dkr.ecr.ap-east-1.amazonaws.com/kubesphere/java-11-centos7:v3.2.0 runtimeArtifacts: - source: /deployments runtimeImage: 969049650220.dkr.ecr.ap-east-1.amazonaws.com/kubesphere/java-11-runtime:v3.2.0 defaultBaseImage: 969049650220.dkr.ecr.ap-east-1.amazonaws.com/kubesphere/java-8-centos7:v3.2.0 description: This is a builder template for Java builds whose result can be run directly without any further application server.It's suited ideally for microservices with a flat classpath (including \"far jars\") environment: - defaultValue: \"\" description: Arguments to use when calling Maven, replacing the default package hawt-app:build -DskipTests -e. Please be sure to run the hawt-app:build goal (when not already bound to the package execution phase), otherwise the startup scripts won't work. key: MAVEN_ARGS required: false type: string - defaultValue: \"\" description: Additional Maven arguments, useful for temporary adding arguments like -X or -am -pl . key: MAVEN_ARGS_APPEND required: false type: string - defaultValue: \"\" description: With Repositories you specify from which locations you want to download certain artifacts, such as dependencies and maven-plugins. key: MAVEN_MIRROR_URL required: false type: string - defaultValue: \"\" description: If set then the Maven repository is removed after the artifact is built. This is useful for keeping the created application image small, but prevents incremental builds. The default is false key: MAVEN_CLEAR_REPO required: false type: boolean - defaultValue: \"\" description: Path to target/ where the jar files are created for multi module builds. These are added to ${MAVEN_ARGS} key: ARTIFACT_DIR required: false type: string - defaultValue: \"\" description: Arguments to use when copying artifacts from the output dir to the application dir. Useful to specify which artifacts will be part of the image. It defaults to -r hawt-app/* when a hawt-app dir is found on the build directory, otherwise jar files only will be included (*.jar). key: ARTIFACT_COPY_ARGS required: false type: string - defaultValue: \"\" description: the directory where the application resides. All paths in your application are","date":"2022-07-21","objectID":"/s2i%E8%87%AA%E5%AE%9A%E4%B9%89%E6%9E%84%E5%BB%BA%E5%99%A8%E5%92%8C%E6%A8%A1%E6%9D%BF/:0:6","tags":["kubebuilder","Operator"],"title":"Kubesphere S2I自定义构建器和模板","uri":"/s2i%E8%87%AA%E5%AE%9A%E4%B9%89%E6%9E%84%E5%BB%BA%E5%99%A8%E5%92%8C%E6%A8%A1%E6%9D%BF/"},{"categories":["Operator"],"content":"参考资料 s2i-base-container s2i-java-container s2i-java-runtimeImage ","date":"2022-07-21","objectID":"/s2i%E8%87%AA%E5%AE%9A%E4%B9%89%E6%9E%84%E5%BB%BA%E5%99%A8%E5%92%8C%E6%A8%A1%E6%9D%BF/:0:7","tags":["kubebuilder","Operator"],"title":"Kubesphere S2I自定义构建器和模板","uri":"/s2i%E8%87%AA%E5%AE%9A%E4%B9%89%E6%9E%84%E5%BB%BA%E5%99%A8%E5%92%8C%E6%A8%A1%E6%9D%BF/"},{"categories":["VPN"],"content":"安装ocserv apt-get install ocserv -y ","date":"2022-07-21","objectID":"/ocserv%E9%83%A8%E7%BD%B2/:0:1","tags":["Openconnect","Ocserv"],"title":"Openconnect部署","uri":"/ocserv%E9%83%A8%E7%BD%B2/"},{"categories":["VPN"],"content":"申请免费证书 修改dns指向服务器 生成证书 certbot certonly --standalone --preferred-challenges http --agree-tos --email kbsonlong@gmail.com -d myvpn.alongparty.cn # 续签证书 certbot renew --quiet --no-self-upgrade ","date":"2022-07-21","objectID":"/ocserv%E9%83%A8%E7%BD%B2/:0:2","tags":["Openconnect","Ocserv"],"title":"Openconnect部署","uri":"/ocserv%E9%83%A8%E7%BD%B2/"},{"categories":["VPN"],"content":"修改配置 auth = \"plain[/etc/ocserv/ocpasswd]\" tcp-port = 443 udp-port = 443 run-as-user = nobody run-as-group = daemon socket-file = /var/run/ocserv-socket server-cert = /etc/letsencrypt/live/myvpn.alongparty.cn/fullchain.pem server-key = /etc/letsencrypt/live/myvpn.alongparty.cn/privkey.pem isolate-workers = false max-clients = 16 max-same-clients = 2 rate-limit-ms = 100 server-stats-reset-time = 604800 keepalive = 32400 dpd = 90 mobile-dpd = 1800 switch-to-tcp-timeout = 25 try-mtu-discovery = true cert-user-oid = 0.9.2342.19200300.100.1.1 tls-priorities = \"NORMAL:%SERVER_PRECEDENCE:%COMPAT:-VERS-SSL3.0:-VERS-TLS1.0:-VERS-TLS1.1\" auth-timeout = 240 min-reauth-time = 300 max-ban-score = 80 ban-reset-time = 1200 cookie-timeout = 300 deny-roaming = false rekey-time = 172800 rekey-method = ssl use-occtl = true pid-file = /var/run/ocserv.pid device = vpns predictable-ips = true default-domain = example.com ipv4-network = 10.255.255.0 ipv4-netmask = 255.255.255.0 tunnel-all-dns = true dns = 8.8.8.8 dns = 4.2.2.4 dns = 2001:4860:4860::8888 dns = 2001:4860:4860::8844 ping-leases = false cisco-client-compat = true dtls-legacy = true ","date":"2022-07-21","objectID":"/ocserv%E9%83%A8%E7%BD%B2/:0:3","tags":["Openconnect","Ocserv"],"title":"Openconnect部署","uri":"/ocserv%E9%83%A8%E7%BD%B2/"},{"categories":["VPN"],"content":"1.首先添加两个带分组的用户 ocpasswd -c /etc/ocserv/ocpasswd -g gruop1 user1 ocpasswd -c /etc/ocserv/ocpasswd -g gruop2 user2 ","date":"2022-07-21","objectID":"/openconnect%E8%AE%BE%E7%BD%AE%E7%94%A8%E6%88%B7%E7%BB%84%E5%AE%9E%E7%8E%B0%E5%A4%9A%E8%B7%AF%E7%94%B1/:0:1","tags":["Openconnect","Ocserv"],"title":"Openconnect设置用户组实现多路由表","uri":"/openconnect%E8%AE%BE%E7%BD%AE%E7%94%A8%E6%88%B7%E7%BB%84%E5%AE%9E%E7%8E%B0%E5%A4%9A%E8%B7%AF%E7%94%B1/"},{"categories":["VPN"],"content":"2.添加创建路由表组 mkdir /etc/ocserv/group echo -e \"route = 10.10.0.0/255.255.255.0\" \u003e\u003e /etc/ocserv/group/group1 echo -e \"no-route = 211.80.0.0/255.240.0.0\" \u003e\u003e /etc/ocserv/group/group2 以上连个路由表是演示group1和group2随便写的 请自行添加路由规则 此外路由表里还可以写DNS 短线时间的参数 ","date":"2022-07-21","objectID":"/openconnect%E8%AE%BE%E7%BD%AE%E7%94%A8%E6%88%B7%E7%BB%84%E5%AE%9E%E7%8E%B0%E5%A4%9A%E8%B7%AF%E7%94%B1/:0:2","tags":["Openconnect","Ocserv"],"title":"Openconnect设置用户组实现多路由表","uri":"/openconnect%E8%AE%BE%E7%BD%AE%E7%94%A8%E6%88%B7%E7%BB%84%E5%AE%9E%E7%8E%B0%E5%A4%9A%E8%B7%AF%E7%94%B1/"},{"categories":["VPN"],"content":"3.添加新的配置到ocserv.conf config-per-group = /etc/ocserv/group/ default-group-config = /etc/ocserv/group/group1 #如果创建用户的时候不分组 group1就是默认分组 用的就是group1的路由表 default-select-group = group1 #如果创建用户的时候不分组 group1就是默认分组 用的就是group1的路由表 auto-select-group = false ","date":"2022-07-21","objectID":"/openconnect%E8%AE%BE%E7%BD%AE%E7%94%A8%E6%88%B7%E7%BB%84%E5%AE%9E%E7%8E%B0%E5%A4%9A%E8%B7%AF%E7%94%B1/:0:3","tags":["Openconnect","Ocserv"],"title":"Openconnect设置用户组实现多路由表","uri":"/openconnect%E8%AE%BE%E7%BD%AE%E7%94%A8%E6%88%B7%E7%BB%84%E5%AE%9E%E7%8E%B0%E5%A4%9A%E8%B7%AF%E7%94%B1/"},{"categories":["VPN"],"content":"4.完整配置 auth = \"plain[/etc/ocserv/ocpasswd]\" tcp-port = 443 udp-port = 443 run-as-user = nobody run-as-group = daemon socket-file = /var/run/ocserv-socket server-cert = /etc/letsencrypt/live/myvpn.alongparty.cn/fullchain.pem server-key = /etc/letsencrypt/live/myvpn.alongparty.cn/privkey.pem isolate-workers = false max-clients = 16 max-same-clients = 2 rate-limit-ms = 100 server-stats-reset-time = 604800 keepalive = 32400 dpd = 90 mobile-dpd = 1800 switch-to-tcp-timeout = 25 try-mtu-discovery = true cert-user-oid = 0.9.2342.19200300.100.1.1 tls-priorities = \"NORMAL:%SERVER_PRECEDENCE:%COMPAT:-VERS-SSL3.0:-VERS-TLS1.0:-VERS-TLS1.1\" auth-timeout = 240 min-reauth-time = 300 max-ban-score = 80 ban-reset-time = 1200 cookie-timeout = 300 deny-roaming = false rekey-time = 172800 rekey-method = ssl use-occtl = true pid-file = /var/run/ocserv.pid device = vpns predictable-ips = true default-domain = example.com ipv4-network = 10.255.255.0 ipv4-netmask = 255.255.255.0 tunnel-all-dns = true dns = 8.8.8.8 dns = 4.2.2.4 dns = 2001:4860:4860::8888 dns = 2001:4860:4860::8844 ping-leases = false config-per-group = /etc/ocserv/group/ default-group-config = /etc/ocserv/group/default default-select-group = default auto-select-group = false cisco-client-compat = true dtls-legacy = true # cat /etc/ocserv/group/default route = 10.48.16.124/32 # cat /etc/ocserv/ocpasswd sysop01:sysop:$5$R7HQXGtZ1MpB.N82$iNf5viGa/XT/qLjfpFhPNqlvdEyw5KKaKZvK2jIVEG4 ","date":"2022-07-21","objectID":"/openconnect%E8%AE%BE%E7%BD%AE%E7%94%A8%E6%88%B7%E7%BB%84%E5%AE%9E%E7%8E%B0%E5%A4%9A%E8%B7%AF%E7%94%B1/:0:4","tags":["Openconnect","Ocserv"],"title":"Openconnect设置用户组实现多路由表","uri":"/openconnect%E8%AE%BE%E7%BD%AE%E7%94%A8%E6%88%B7%E7%BB%84%E5%AE%9E%E7%8E%B0%E5%A4%9A%E8%B7%AF%E7%94%B1/"},{"categories":["VPN"],"content":"修改认证方式 # vim /etc/ocserv/ocserv.conf auth = \"plain[passwd=/etc/ocserv/ocpasswd,otp=/etc/ocserv/ocserv.otp]\" ","date":"2022-07-21","objectID":"/openconnect%E5%8F%8C%E5%9B%A0%E7%B4%A0%E8%AE%A4%E8%AF%81/:0:1","tags":["Openconnect","Ocserv"],"title":"Openconnect双因素认证","uri":"/openconnect%E5%8F%8C%E5%9B%A0%E7%B4%A0%E8%AE%A4%E8%AF%81/"},{"categories":["VPN"],"content":"配置pam echo \"auth requisite pam_oath.so debug usersfile=/etc/ocserv/ocserv.otp window=20\" \u003e\u003e /etc/pam.d/ocserv ","date":"2022-07-21","objectID":"/openconnect%E5%8F%8C%E5%9B%A0%E7%B4%A0%E8%AE%A4%E8%AF%81/:0:2","tags":["Openconnect","Ocserv"],"title":"Openconnect双因素认证","uri":"/openconnect%E5%8F%8C%E5%9B%A0%E7%B4%A0%E8%AE%A4%E8%AF%81/"},{"categories":["VPN"],"content":"创建用户OTP username=\"zengshenglong\" company=\"Company\" site_name=\"Site\" key=$(head -c 16 /dev/urandom |xxd -c 256 -ps) echo \"HOTP/T30 ${username} - ${key}\" \u003e\u003e/etc/ocserv/ocserv.otp oathtool --totp -w 5 $key secret=$(echo 0x${key}|xxd -r -c 256|base32) echo \"otpauth://hotp/${username}@${site_name}?secret=${secret}\u0026issuer=${company}\" | qrencode -o - -t UTF8 echo \"https://www.google.com/chart?chs=200x200\u0026chld=M|0\u0026cht=qr\u0026chl=otpauth://hotp/${username}@${site_name}?secret=$(echo ${secret}|cut -d = -f 1)\u0026issuer=${company}\" ","date":"2022-07-21","objectID":"/openconnect%E5%8F%8C%E5%9B%A0%E7%B4%A0%E8%AE%A4%E8%AF%81/:0:3","tags":["Openconnect","Ocserv"],"title":"Openconnect双因素认证","uri":"/openconnect%E5%8F%8C%E5%9B%A0%E7%B4%A0%E8%AE%A4%E8%AF%81/"},{"categories":["VPN"],"content":"完整配置 auth = \"plain[passwd=/etc/ocserv/ocpasswd,otp=/etc/ocserv/ocserv.otp]\" tcp-port = 443 udp-port = 443 run-as-user = nobody run-as-group = daemon socket-file = /var/run/ocserv-socket server-cert = /etc/letsencrypt/live/myvpn.alongparty.cn/fullchain.pem server-key = /etc/letsencrypt/live/myvpn.alongparty.cn/privkey.pem isolate-workers = false banner = \"Welcome PCI DSS environment, please proceed with caution ! !\" pre-login-banner = \"You will enter the PCI DSS environment, please proceed with caution ! !\" max-clients = 16 max-same-clients = 2 rate-limit-ms = 100 server-stats-reset-time = 604800 keepalive = 32400 dpd = 90 mobile-dpd = 1800 switch-to-tcp-timeout = 25 try-mtu-discovery = true cert-user-oid = 0.9.2342.19200300.100.1.1 tls-priorities = \"NORMAL:%SERVER_PRECEDENCE:%COMPAT:-VERS-SSL3.0:-VERS-TLS1.0:-VERS-TLS1.1\" auth-timeout = 240 min-reauth-time = 300 max-ban-score = 80 ban-reset-time = 1200 cookie-timeout = 300 deny-roaming = false rekey-time = 172800 rekey-method = ssl use-occtl = true pid-file = /var/run/ocserv.pid device = vpns predictable-ips = true default-domain = example.com ipv4-network = 10.255.255.0 ipv4-netmask = 255.255.255.0 tunnel-all-dns = true dns = 8.8.8.8 dns = 4.2.2.4 dns = 2001:4860:4860::8888 dns = 2001:4860:4860::8844 ping-leases = false config-per-group = /etc/ocserv/group default-group-config = /etc/ocserv/group/default default-select-group = default auto-select-group = false cisco-client-compat = true dtls-legacy = true ","date":"2022-07-21","objectID":"/openconnect%E5%8F%8C%E5%9B%A0%E7%B4%A0%E8%AE%A4%E8%AF%81/:0:4","tags":["Openconnect","Ocserv"],"title":"Openconnect双因素认证","uri":"/openconnect%E5%8F%8C%E5%9B%A0%E7%B4%A0%E8%AE%A4%E8%AF%81/"},{"categories":["golang"],"content":"初始化demo项目 mkdir -p gin-middleware-demo cd gin-middleware-demo go mod init github.com/kbsonlong/gin-middleware-demo middleware/middle.go package middleware import ( \"fmt\" \"time\" \"github.com/gin-gonic/gin\" ) func MdOne() gin.HandlerFunc { return func(c *gin.Context) { fmt.Println(\"开始执行第一个 gin 中间件:\" + time.Now().String()) c.Next() fmt.Println(\"第一个 gin 中间件返回内容:\" + time.Now().String()) } } func MdTwo() gin.HandlerFunc { return func(c *gin.Context) { fmt.Println(\"开始执行第二个 gin 中间件:\" + time.Now().String()) c.Next() fmt.Println(\"第二个 gin 中间件返回内容:\" + time.Now().String()) } } func MdThree() gin.HandlerFunc { return func(c *gin.Context) { fmt.Println(\"开始执行第三个 gin 中间件:\" + time.Now().String()) c.Next() fmt.Println(\"第三个 gin 中间件返回内容:\" + time.Now().String()) } } routers/router.go package routers import ( \"github.com/gin-gonic/gin\" \"github.com/kbsonlong/gin-middleware-demo/middleware\" ) func InitRouter() *gin.Engine { r := gin.New() r.Use(middleware.MdOne()) r.Use(middleware.MdTwo()) r.Use(middleware.MdThree()) gin.SetMode(\"debug\") r.GET(\"/\", func(ctx *gin.Context) { ctx.JSON(200, gin.H{ \"message\": \"test\", }) }) return r } main.go package main import ( \"github.com/kbsonlong/gin-middleware-demo/routers\" ) func main() { router := routers.InitRouter() router.Run() } ","date":"2022-07-08","objectID":"/gin%E4%B8%AD%E9%97%B4%E4%BB%B6%E6%89%A7%E8%A1%8C%E9%A1%BA%E5%BA%8F/:0:1","tags":["Gin"],"title":"了解Gin中间件执行顺序","uri":"/gin%E4%B8%AD%E9%97%B4%E4%BB%B6%E6%89%A7%E8%A1%8C%E9%A1%BA%E5%BA%8F/"},{"categories":["golang"],"content":"运行测试 go mod tidy go run main.go [GIN-debug] [WARNING] Running in \"debug\" mode. Switch to \"release\" mode in production. - using env: export GIN_MODE=release - using code: gin.SetMode(gin.ReleaseMode) [GIN-debug] GET / --\u003e github.com/kbsonlong/gin-middleware-demo/routers.InitRouter.func1 (4 handlers) [GIN-debug] [WARNING] You trusted all proxies, this is NOT safe. We recommend you to set a value. Please check https://pkg.go.dev/github.com/gin-gonic/gin#readme-don-t-trust-all-proxies for details. [GIN-debug] Environment variable PORT is undefined. Using port :8080 by default [GIN-debug] Listening and serving HTTP on :8080 另一终端执行 curl 127.0.0.1:8080 {\"message\":\"test\"} 可以看到控制台输出内容 开始执行第一个 gin 中间件:2022-07-08 17:18:57.499033 +0800 CST m=+3.029327751 开始执行第二个 gin 中间件:2022-07-08 17:18:57.499215 +0800 CST m=+3.029509918 开始执行第三个 gin 中间件:2022-07-08 17:18:57.499218 +0800 CST m=+3.029513043 第三个 gin 中间件返回内容:2022-07-08 17:18:57.499295 +0800 CST m=+3.029589876 第二个 gin 中间件返回内容:2022-07-08 17:18:57.499298 +0800 CST m=+3.029593335 第一个 gin 中间件返回内容:2022-07-08 17:18:57.4993 +0800 CST m=+3.029594876 调整中间件Use顺序 ...... r.Use(middleware.MdOne()) r.Use(middleware.MdThree()) r.Use(middleware.MdTwo()) gin.SetMode(\"debug\") ...... 第二个中间件和第三个中间件顺序对调 开始执行第一个 gin 中间件:2022-07-08 17:20:57.702026 +0800 CST m=+2.567931210 开始执行第三个 gin 中间件:2022-07-08 17:20:57.702215 +0800 CST m=+2.568120210 开始执行第二个 gin 中间件:2022-07-08 17:20:57.702218 +0800 CST m=+2.568123543 第二个 gin 中间件返回内容:2022-07-08 17:20:57.702305 +0800 CST m=+2.568210460 第三个 gin 中间件返回内容:2022-07-08 17:20:57.702308 +0800 CST m=+2.568214043 第一个 gin 中间件返回内容:2022-07-08 17:20:57.702311 +0800 CST m=+2.568216293 可以看到第三个中间件在第二个中间件之前执行 注释第三个中间件Next func MdThree() gin.HandlerFunc { return func(c *gin.Context) { fmt.Println(\"开始执行第三个 gin 中间件:\" + time.Now().String()) // c.Next() fmt.Println(\"第三个 gin 中间件返回内容:\" + time.Now().String()) } } 开始执行第一个 gin 中间件:2022-07-08 17:22:43.132983 +0800 CST m=+4.061931376 开始执行第三个 gin 中间件:2022-07-08 17:22:43.133126 +0800 CST m=+4.062074668 第三个 gin 中间件返回内容:2022-07-08 17:22:43.133128 +0800 CST m=+4.062076751 开始执行第二个 gin 中间件:2022-07-08 17:22:43.13313 +0800 CST m=+4.062078210 第二个 gin 中间件返回内容:2022-07-08 17:22:43.133203 +0800 CST m=+4.062151335 第一个 gin 中间件返回内容:2022-07-08 17:22:43.133205 +0800 CST m=+4.062153460 ","date":"2022-07-08","objectID":"/gin%E4%B8%AD%E9%97%B4%E4%BB%B6%E6%89%A7%E8%A1%8C%E9%A1%BA%E5%BA%8F/:0:2","tags":["Gin"],"title":"了解Gin中间件执行顺序","uri":"/gin%E4%B8%AD%E9%97%B4%E4%BB%B6%E6%89%A7%E8%A1%8C%E9%A1%BA%E5%BA%8F/"},{"categories":["golang"],"content":"总结 Gin中间件的调用顺序与Use顺序有关，代码运行顺序和Next前后顺序有关。 Next之前代码先进先出 Next之后代码后进先出 没有引用Next代码直接运行 ","date":"2022-07-08","objectID":"/gin%E4%B8%AD%E9%97%B4%E4%BB%B6%E6%89%A7%E8%A1%8C%E9%A1%BA%E5%BA%8F/:0:3","tags":["Gin"],"title":"了解Gin中间件执行顺序","uri":"/gin%E4%B8%AD%E9%97%B4%E4%BB%B6%E6%89%A7%E8%A1%8C%E9%A1%BA%E5%BA%8F/"},{"categories":null,"content":"Hey Welcome here 👋 ","date":"2022-05-24","objectID":"/about/:1:0","tags":null,"title":"","uri":"/about/"},{"categories":null,"content":"正在学习 ","date":"2022-05-24","objectID":"/about/:2:0","tags":null,"title":"","uri":"/about/"},{"categories":null,"content":"Service Mesh ","date":"2022-05-24","objectID":"/about/:2:1","tags":null,"title":"","uri":"/about/"},{"categories":null,"content":"Vue ","date":"2022-05-24","objectID":"/about/:2:2","tags":null,"title":"","uri":"/about/"},{"categories":null,"content":"Go ","date":"2022-05-24","objectID":"/about/:2:3","tags":null,"title":"","uri":"/about/"},{"categories":null,"content":"汇编和工具: 🛠 Check for a detailed stats here :point_right: Sourcerer ","date":"2022-05-24","objectID":"/about/:3:0","tags":null,"title":"","uri":"/about/"},{"categories":["云原生"],"content":"1限流 ","date":"2022-05-23","objectID":"/istio%E9%98%B2%E6%95%85%E9%9A%9C%E5%88%A9%E5%99%A8/:0:0","tags":["istio"],"title":"Istio防故障利器","uri":"/istio%E9%98%B2%E6%95%85%E9%9A%9C%E5%88%A9%E5%99%A8/"},{"categories":["云原生"],"content":"1.1什么是限流 ​ 举个例子，比如我们有个桶，桶里有两个开关，一个往外出水，一个网内注水，当出水的速度慢于注水的速度时，到一定时间水就会从桶里溢出。如果我们限制注水速率，就可以防止水从桶里溢出，这就是限流。 ​ 具体到软件层面，我们把请求速率看做是注水，把系统cpu，内存等资源看做是放水，当请求速率过快，消耗太多资源时系统就可能崩溃。软件限流就是限制tps或qps指标，以达到保护系统的目的，虽然可能部分用户无法服务，但是系统整体还是健康的，还可以对外部提供服务，不是整体挂掉。 ","date":"2022-05-23","objectID":"/istio%E9%98%B2%E6%95%85%E9%9A%9C%E5%88%A9%E5%99%A8/:1:0","tags":["istio"],"title":"Istio防故障利器","uri":"/istio%E9%98%B2%E6%95%85%E9%9A%9C%E5%88%A9%E5%99%A8/"},{"categories":["云原生"],"content":"1.2限流算法 1.2.1漏桶算法 就像一个漏斗以下，下面小，上面大。漏桶流出的速率被限制在比较小的范围，当漏桶满时，漏桶就会溢出，进来的请求就会被抛弃掉。特别是应对突发流量，漏桶的速率是恒定的，这样可以有效防止应突发流量导致系统崩溃。 ","date":"2022-05-23","objectID":"/istio%E9%98%B2%E6%95%85%E9%9A%9C%E5%88%A9%E5%99%A8/:2:0","tags":["istio"],"title":"Istio防故障利器","uri":"/istio%E9%98%B2%E6%95%85%E9%9A%9C%E5%88%A9%E5%99%A8/"},{"categories":["云原生"],"content":"1.2.2令牌桶算法 令牌桶算法的原理，关键在令牌，它是指往桶里以一个不变的速率放入令牌，当有请求时，如果桶里有令牌，请求就消费一个令牌，请求继续进行；当请求到来，桶里没有令牌时，请求就会被抛弃掉，拒绝服务；当桶里的令牌满时，令牌就会被抛弃掉。 ","date":"2022-05-23","objectID":"/istio%E9%98%B2%E6%95%85%E9%9A%9C%E5%88%A9%E5%99%A8/:2:1","tags":["istio"],"title":"Istio防故障利器","uri":"/istio%E9%98%B2%E6%95%85%E9%9A%9C%E5%88%A9%E5%99%A8/"},{"categories":["云原生"],"content":"1.2.3计数器算法 计数器算法是指一段时间设置一个计数器，当有请求时计数器就加一，请求继续进行；在技术器时间范围内，当计数器数值超过指定值，请求就被拒绝；当时间范围结束，就重置计数器。技术器算法有个缺陷，就是如果计数器时间是1分钟，当前1秒来了大量请求，讲技术器用完了，后续59秒时间就没法提供服务。 ","date":"2022-05-23","objectID":"/istio%E9%98%B2%E6%95%85%E9%9A%9C%E5%88%A9%E5%99%A8/:2:2","tags":["istio"],"title":"Istio防故障利器","uri":"/istio%E9%98%B2%E6%95%85%E9%9A%9C%E5%88%A9%E5%99%A8/"},{"categories":["云原生"],"content":"1.2实操 ","date":"2022-05-23","objectID":"/istio%E9%98%B2%E6%95%85%E9%9A%9C%E5%88%A9%E5%99%A8/:3:0","tags":["istio"],"title":"Istio防故障利器","uri":"/istio%E9%98%B2%E6%95%85%E9%9A%9C%E5%88%A9%E5%99%A8/"},{"categories":["云原生"],"content":"1.2.1 http 1.2.1.1单集群 istio部署和bookinfo实例部署大家自行完成，都看这种深度的文章了这个应该不是事。 1.2.1.1.1集群内服务限流 1.2.1.1.1.1本地限流 cat \u003c\u003cEOF \u003e envoyfilter-local-rate-limit.yaml apiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: filter-local-ratelimit-svc spec: workloadSelector: labels: app: productpage configPatches: - applyTo: HTTP_FILTER match: listener: filterChain: filter: name: \"envoy.filters.network.http_connection_manager\" patch: operation: INSERT_BEFORE value: name: envoy.filters.http.local_ratelimit typed_config: \"@type\": type.googleapis.com/udpa.type.v1.TypedStruct type_url: type.googleapis.com/envoy.extensions.filters.http.local_ratelimit.v3.LocalRateLimit value: stat_prefix: http_local_rate_limiter token_bucket: max_tokens: 10 tokens_per_fill: 10 fill_interval: 60s filter_enabled: runtime_key: local_rate_limit_enabled default_value: numerator: 100 denominator: HUNDRED filter_enforced: runtime_key: local_rate_limit_enforced default_value: numerator: 100 denominator: HUNDRED response_headers_to_add: - append: false header: key: x-local-rate-limit value: 'true' EOF kubectl apply -f envoyfilter-local-rate-limit.yaml -n istio 说明：本地限流需要通过EnvoyFilter来实现，他不会请求外部服务，在envoy内部实现支持，是一个令牌桶的算法。http filter的名称必须是envoy.filters.http.local_ratelimit，type和typeurl是固定的，stat_prefix可以随便改，表示生成stat的指标前缀。token_bucket配置令牌桶，max_tokens表示最大令牌数量，tokens_per_fill表示每次填充的令牌数量，fill_interval表示填充令牌的间隔。filter_enabled表示启用但不是强制，filter_enforced表示强制，可以配置百分比。response_headers_to_add修改响应头信息，append为false表示修改，true表示添加。runtime_key 运行时的key，具体有啥用不清楚。 执行压测： [root@node01 45]# go-stress-testing -c 10 -n 10000 -u http://192.168.229.134:30945/productpage 开始启动 并发数:10 请求数:10000 请求参数: request: form:http url:http://192.168.229.134:30945/productpage method:GET headers:map[] data: verify:statusCode timeout:30s debug:false ─────┬───────┬───────┬───────┬────────┬────────┬────────┬────────┬────────┬────────┬──────── 耗时│ 并发数│ 成功数│ 失败数│ qps │最长耗时│最短耗时│平均耗时│下载字节│字节每秒│ 错误码 ─────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┼────────┼────────┼──────── 1s│ 7│ 2│ 761│ 2.94│ 124.68│ 1.98│ 3406.97│ 21,476│ 21,470│200:2;429:761 2s│ 10│ 5│ 1636│ 2.55│ 1788.46│ 1.98│ 3928.11│ 52,771│ 26,383│200:5;429:1636 3s│ 10│ 5│ 2962│ 1.70│ 1788.46│ 1.04│ 5871.68│ 76,639│ 25,545│200:5;429:2962 4s│ 10│ 5│ 4459│ 1.28│ 1788.46│ 1.04│ 7810.78│ 103,585│ 25,896│200:5;429:4459 429 Too Many Requests (太多请求) 当你需要限制客户端请求某个服务的数量，也就是限制请求速度时，该状态码就会非常有用 清理： kubectl delete envoyfilter filter-local-ratelimit-svc -n istio 1.2.1.1.1.2全局限流 部署ratelimit 1创建cm cat \u003c\u003c EOF \u003e ratelimit-config.yaml apiVersion: v1 kind: ConfigMap metadata: name: ratelimit-config data: config.yaml: | domain: productpage-ratelimit descriptors: - key: PATH value: \"/productpage\" rate_limit: unit: minute requests_per_unit: 1 - key: PATH rate_limit: unit: minute requests_per_unit: 100 EOF kubectl apply -f ratelimit-config.yaml -n istio 说明: 这个configmap是限速服务用到的配置文件，他是envoy v3版本的限速格式。domain是域名，他会在envoyfilter中被引用，descriptors的PATH,表示请求的路径可以有多个值，rate_limit配置限速配额，这里productpage配了1分钟1个请求，其他url是1分钟100个请求 2创建限速服务deployment cat \u003c\u003c EOF \u003e ratelimit-deploy.yaml apiVersion: v1 kind: Service metadata: name: redis labels: app: redis spec: ports: - name: redis port: 6379 selector: app: redis --- apiVersion: apps/v1 kind: Deployment metadata: name: redis spec: replicas: 1 selector: matchLabels: app: redis template: metadata: labels: app: redis spec: containers: - image: redis:alpine imagePullPolicy: Always name: redis ports: - name: redis containerPort: 6379 restartPolicy: Always serviceAccountName: \"\" --- apiVersion: v1 kind: Service metadata: name: ratelimit labels: app: ratelimit spec: ports: - name: http-port port: 8080 targetPort: 8080 protocol: TCP - name: grpc-port port: 8081 targetPort: 8081 protocol: TCP - name: http-debug port: 6070 targetPort: 6070 protocol: TCP selector: app: ratelimit --- apiVersion: apps/v1 kind: Deployment metadata: name: ratelimit spec: replicas: 1 selector: matchLabels: app: ratelimit strategy: type: Recreate template: metadata: labe","date":"2022-05-23","objectID":"/istio%E9%98%B2%E6%95%85%E9%9A%9C%E5%88%A9%E5%99%A8/:3:1","tags":["istio"],"title":"Istio防故障利器","uri":"/istio%E9%98%B2%E6%95%85%E9%9A%9C%E5%88%A9%E5%99%A8/"},{"categories":null,"content":"kbsonlong's friends","date":"2022-05-23","objectID":"/friends/","tags":null,"title":"","uri":"/friends/"},{"categories":null,"content":"Base info - nickname: 蜷缩的蜗牛 avatar: images/20220524130401.png url: https://www.alongparty.cn description: 运维自动化 ","date":"2022-05-23","objectID":"/friends/:1:0","tags":null,"title":"","uri":"/friends/"},{"categories":null,"content":"Friendly Reminder Notice If you want to exchange link, please leave a comment in the above format. (personal non-commercial blogs / websites only)  Website failure, stop maintenance and improper content may be unlinked! Those websites that do not respect other people’s labor achievements, reprint without source, or malicious acts, please do not come to exchange. ","date":"2022-05-23","objectID":"/friends/:2:0","tags":null,"title":"","uri":"/friends/"},{"categories":["云原生"],"content":"压测大纲 压测的必要性 压测部署架构图 环境准备 ","date":"2022-05-23","objectID":"/istio%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:0:0","tags":["istio"],"title":"Istio性能测试","uri":"/istio%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["云原生"],"content":"部署 Istio ","date":"2022-05-23","objectID":"/istio%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:1:0","tags":["istio"],"title":"Istio性能测试","uri":"/istio%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["云原生"],"content":"部署监控组件 ","date":"2022-05-23","objectID":"/istio%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:2:0","tags":["istio"],"title":"Istio性能测试","uri":"/istio%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["云原生"],"content":"部署压测服务 ","date":"2022-05-23","objectID":"/istio%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:3:0","tags":["istio"],"title":"Istio性能测试","uri":"/istio%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["云原生"],"content":"部署压测工具 性能压测 压测报告","date":"2022-05-23","objectID":"/istio%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/:4:0","tags":["istio"],"title":"Istio性能测试","uri":"/istio%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"categories":["hugo"],"content":"创建Github Actions流水线 mkdir -p .github/workflows/ touch hugo.yaml 发布到本仓库 name: GitHub Pages Deploy Local REPOSITORY on: push: branches: - master # Set a branch to deploy pull_request: jobs: deploy: runs-on: ubuntu-20.04 concurrency: group: ${{ github.workflow }}-${{ github.ref }} steps: - uses: actions/checkout@v3 with: submodules: true # Fetch Hugo themes (true OR recursive) fetch-depth: 1 # Fetch all history for .GitInfo and .Lastmod - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: '0.91.2' extended: true - name: Build run: hugo --minify - name: Deploy uses: peaceiris/actions-gh-pages@v3 with: github_token: ${{ secrets.GITHUB_TOKEN }} publish_dir: ./public 发布到另外仓库 name: CI #自动化的名称 on: push: # push的时候触发 branches: # 那些分支需要触发 - master jobs: deploy: runs-on: ubuntu-20.04 steps: - uses: actions/checkout@v2 with: submodules: true # Fetch Hugo themes (true OR recursive) fetch-depth: 1 # Fetch all history for .GitInfo and .Lastmod - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: 'latest' extended: true - name: Build run: hugo --minify - name: Deploy uses: peaceiris/actions-gh-pages@v3 with: personal_token: ${{ secrets.EXTERNAL_REPOSITORY_TOKEN }} publish_dir: ./public external_repository: kbsonlong/devops.alongparty.cn 注意: 发布到本仓库github_token不需要手动创建GITHUB_TOKEN, 发布到其他仓库personal_token需要提前创建并配置secret变量 创建SSH证书 ssh-keygen -t rsa -b 4096 -C \"$(git config user.email)\" -f gh-pages -N \"\" 配置SSH公钥 配置SSH私钥 使用SSH私钥发布 name: CI #自动化的名称 on: push: # push的时候触发 branches: # 那些分支需要触发 - master jobs: deploy: runs-on: ubuntu-20.04 steps: - uses: actions/checkout@v2 with: submodules: true # Fetch Hugo themes (true OR recursive) fetch-depth: 1 # Fetch all history for .GitInfo and .Lastmod - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: 'latest' extended: true - name: Build run: hugo --minify - name: Deploy with PRIVATE KEY uses: peaceiris/actions-gh-pages@v3 with: cname: devops.alongparty.cn env: ACTIONS_DEPLOY_KEY: ${{ secrets.HUGO_DEPLOY_PRIVATE_KEY }} EXTERNAL_REPOSITORY: kbsonlong/devops.alongparty.cn PUBLISH_BRANCH: gh-pages PUBLISH_DIR: ./public 注意: 部署到另外仓库时ssh私钥配置在源仓库, ssh公钥配置在目标仓库或者全局 ","date":"2022-05-23","objectID":"/github-actions%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2hugo%E5%8D%9A%E5%AE%A2/:0:1","tags":["github actions"],"title":"Github Actions自动部署hugo博客","uri":"/github-actions%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2hugo%E5%8D%9A%E5%AE%A2/"},{"categories":["hugo"],"content":"创建GITHUB Personal TOKEN ","date":"2022-05-23","objectID":"/github-actions%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2hugo%E5%8D%9A%E5%AE%A2/:0:2","tags":["github actions"],"title":"Github Actions自动部署hugo博客","uri":"/github-actions%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2hugo%E5%8D%9A%E5%AE%A2/"},{"categories":["hugo"],"content":"创建仓库Secrets变量 ","date":"2022-05-23","objectID":"/github-actions%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2hugo%E5%8D%9A%E5%AE%A2/:0:3","tags":["github actions"],"title":"Github Actions自动部署hugo博客","uri":"/github-actions%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2hugo%E5%8D%9A%E5%AE%A2/"},{"categories":["hugo"],"content":"Github Page配置自定义域名 echo \"\u003c自定义域名\u003e\" \u003estatic/CNAME ","date":"2022-05-23","objectID":"/github-actions%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2hugo%E5%8D%9A%E5%AE%A2/:0:4","tags":["github actions"],"title":"Github Actions自动部署hugo博客","uri":"/github-actions%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2hugo%E5%8D%9A%E5%AE%A2/"},{"categories":["hugo"],"content":"Deploy时过滤静态文件 - name: Deploy uses: peaceiris/actions-gh-pages@v3 with: personal_token: ${{ secrets.EXTERNAL_REPOSITORY_TOKEN }} external_repository: kbsonlong/devops.alongparty.cn publish_branch: gh-pages # default: gh-pages publish_dir: ./public exclude_assets: './algolia.json,./*/*.md' # 支持正则过滤,基于编译后的静态文件,根目录publish_dir user_name: 'github-actions[bot]' user_email: 'github-actions[bot]@users.noreply.github.com' commit_message: ${{ github.event.head_commit.message }} tag_name: ${{ steps.prepare_tag.outputs.deploy_tag_name }} tag_message: 'Deployment ${{ steps.prepare_tag.outputs.tag_name }}' ","date":"2022-05-23","objectID":"/github-actions%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2hugo%E5%8D%9A%E5%AE%A2/:0:5","tags":["github actions"],"title":"Github Actions自动部署hugo博客","uri":"/github-actions%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2hugo%E5%8D%9A%E5%AE%A2/"},{"categories":null,"content":"离线 - 蜷缩的蜗牛","date":"0001-01-01","objectID":"/offline/","tags":null,"title":"","uri":"/offline/"},{"categories":null,"content":"Hey Welcome here 👋 ","date":"0001-01-01","objectID":"/offline/:1:0","tags":null,"title":"","uri":"/offline/"},{"categories":null,"content":"目前正在学习Service Mesh ","date":"0001-01-01","objectID":"/offline/:2:0","tags":null,"title":"","uri":"/offline/"},{"categories":null,"content":"汇编和工具: 🛠 Check for a detailed stats here :point_right: Sourcerer ","date":"0001-01-01","objectID":"/offline/:3:0","tags":null,"title":"","uri":"/offline/"}]